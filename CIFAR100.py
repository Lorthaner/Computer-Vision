# -*- coding: utf-8 -*-
"""Cifar_100_Piotrek.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UfF9VO2YdslnAwPOsq50TUHwh7pI23uM
"""

import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as opt
import matplotlib.pyplot as plt
import numpy as np

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

trainset = torchvision.datasets.CIFAR100(root = './sample_data', train = True, download = True, transform = transforms.Compose([transforms.RandomHorizontalFlip(),
                                                                                                                                 transforms.ToTensor(),
                                                                                                                                 transforms.RandomErasing(),
                                                                                                                                 transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]))

testset = torchvision.datasets.CIFAR100(root = './sample_data', train = False, transform = transforms.Compose([transforms.ToTensor(),
                                                                                               transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]))

train_loader = torch.utils.data.DataLoader(trainset, batch_size = 100, shuffle = True)
test_loader = torch.utils.data.DataLoader(testset, batch_size = 100, shuffle = True)

len(testset)

class CIFAR100(nn.Module):
  
  def __init__(self):
    super().__init__()
    self.sequential = nn.Sequential(
        nn.Conv2d(3,32,3, padding = 1),
        nn.ReLU(),
        nn.BatchNorm2d(32),
        nn.Conv2d(32,64,3, padding = 1),
        nn.ReLU(),
        nn.BatchNorm2d(64),
        nn.Conv2d(64,64,3, padding = 1),
        nn.MaxPool2d(2,2),  # 32 , 16 , 16
        nn.Dropout(0.2),
        nn.Conv2d(64,128,3, padding = 1),
        nn.ReLU(),
        nn.BatchNorm2d(128),
        nn.Conv2d(128,128,3, padding = 1),
        nn.ReLU(),
        nn.BatchNorm2d(128),
        nn.Conv2d(128,256,3, padding = 1),
        nn.MaxPool2d(2,2),  # 64, 8 , 8
        nn.Dropout(0.3),
        nn.Conv2d(256,256,3, padding = 1),
        nn.ReLU(),
        nn.BatchNorm2d(256),
        nn.Conv2d(256,512,3, padding = 1),
        nn.ReLU(),
        nn.BatchNorm2d(512),
        nn.Conv2d(512,512,3, padding = 1),
        nn.MaxPool2d(2,2), # 128, 4, 4
        nn.Dropout(0.4),
        nn.Flatten(),
        nn.Linear(512*4*4,512),
        nn.Linear(512,256),
        nn.Linear(256,128),
        nn.Linear(128,100)
    )

  def forward(self,x):
      return self.sequential(x)

model = CIFAR100()

model = model.to(device)

def training_loop(epochs, learning_rate, weight_decay):

    criterion = nn.CrossEntropyLoss()
    optimizer = opt.SGD(model.parameters(),lr = learning_rate, weight_decay=weight_decay)

    for epoch in range(epochs):
      correct = 0
      total = 0
      for i, data in enumerate(train_loader, 0):
              # get the inputs; data is a list of [inputs, labels]
            images, labels = data[0].to(device), data[1].to(device)

            optimizer.zero_grad()
            model_predictions= model(images)
            #print(torch.max(model_predictions,1)[1])
            loss = criterion(model_predictions, labels)
            loss.backward()
            optimizer.step()
            _, predicted = torch.max(model_predictions, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
      accuracy = 100 * correct / total
      print("Train accuracy = {}% , Epoch = {}, Loss = {}".format(accuracy,epoch+1, loss))
      correct_test = 0
      total_test = 0
      correct_k = 0
      with torch.no_grad():
          for data_test in test_loader:
            images_test, labels_test  = data_test[0].to(device), data_test[1].to(device)
            outputs = model(images_test)
            #top 1 calculation
            _,predicted_test = torch.max(outputs.data,1)
            total_test += labels_test.size(0)
            correct_test += (predicted_test == labels_test).sum().item()
            test_accuracy = 100 * correct_test / total_test

            #top 5 calculation

            _, pred = outputs.topk(5, dim = 1, largest = True, sorted = True)
            pred = pred.t()
            correct_top_5 = pred.eq(labels_test.view(1, -1).expand_as(pred))

            correct_k += correct_top_5[:5].reshape(-1).float().sum().item()
            top_5_acc = 100 * correct_k / total_test
            

          print("Test_accuracy: top 1 = {}%".format(test_accuracy))
          print("Test_accuracy: top 5 = {:.3f}%".format(top_5_acc))

training_loop(50,0.01,1e-5)